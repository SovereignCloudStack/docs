"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[60183],{65729:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"scs-0214-v2-k8s-node-distribution","title":"Kubernetes Node Distribution and Availability","description":"Introduction","source":"@site/standards/scs-0214-v2-k8s-node-distribution.md","sourceDirName":".","slug":"/scs-0214-v2-k8s-node-distribution","permalink":"/standards/scs-0214-v2-k8s-node-distribution","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Kubernetes Node Distribution and Availability","type":"Standard","status":"Stable","stabilized_at":"2024-11-21T00:00:00.000Z","replaces":"scs-0214-v1-k8s-node-distribution.md","track":"KaaS"},"sidebar":"standards","previous":{"title":"V1","permalink":"/standards/scs-0214-v1-k8s-node-distribution"},"next":{"title":"W1","permalink":"/standards/scs-0214-w1-k8s-node-distribution-implementation-testing"}}');var i=n(74848),o=n(28453);const r={title:"Kubernetes Node Distribution and Availability",type:"Standard",status:"Stable",stabilized_at:new Date("2024-11-21T00:00:00.000Z"),replaces:"scs-0214-v1-k8s-node-distribution.md",track:"KaaS"},a=void 0,l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Glossary",id:"glossary",level:3},{value:"Motivation",id:"motivation",level:2},{value:"Design Considerations",id:"design-considerations",level:2},{value:"Decision",id:"decision",level:2},{value:"Previous standard versions",id:"previous-standard-versions",level:2}];function c(e){const t={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(t.p,{children:"A Kubernetes instance is provided as a cluster, which consists of a set of machines,\nso-called nodes. A cluster is composed of a control plane and at least one worker node.\nThe control plane manages the worker nodes and therefore the pods in the cluster by making\ndecisions about scheduling, event detection and rights management. Inside the control plane,\nmultiple components exist, which can be duplicated and distributed over multiple nodes\ninside the cluster. Typically, no user workloads are run on these nodes in order to\nseparate the controller component from user workloads, which could pose a security risk."}),"\n",(0,i.jsx)(t.h3,{id:"glossary",children:"Glossary"}),"\n",(0,i.jsx)(t.p,{children:"The following terms are used throughout this document:"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Term"}),(0,i.jsx)(t.th,{children:"Meaning"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Worker"}),(0,i.jsx)(t.td,{children:"Virtual or bare-metal machine, which hosts workloads of customers"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Control Plane"}),(0,i.jsx)(t.td,{children:"Virtual or bare-metal machine, which hosts the container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Machine"}),(0,i.jsx)(t.td,{children:"Virtual or bare-metal entity with computational capabilities"})]})]})]}),"\n",(0,i.jsx)(t.h2,{id:"motivation",children:"Motivation"}),"\n",(0,i.jsx)(t.p,{children:'In normal day-to-day operation, it is not unusual for some operational failures, either\ndue to wear and tear of hardware, software misconfigurations, external problems or\nuser errors. Whichever was the source of such an outage, it always means down-time for\noperations and users and possible even data loss.\nTherefore, a Kubernetes cluster in a productive environment should be distributed over\nmultiple "failure zones" in order to provide fault-tolerance and high availability.\nThis is especially important for the control plane of the cluster, since it contains the\nstate of the whole cluster. A failure of this component could mean an unrecoverable failure\nof the whole cluster.'}),"\n",(0,i.jsx)(t.h2,{id:"design-considerations",children:"Design Considerations"}),"\n",(0,i.jsxs)(t.p,{children:["Most design considerations of this standard follow the previously written Decision Record\n",(0,i.jsx)(t.a,{href:"https://github.com/SovereignCloudStack/standards/blob/main/Standards/scs-0213-v1-k8s-nodes-anti-affinity.md",children:"Kubernetes Nodes Anti Affinity"})," as well as the Kubernetes documents about\n",(0,i.jsx)(t.a,{href:"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/",children:"High Availability"})," and ",(0,i.jsx)(t.a,{href:"https://kubernetes.io/docs/setup/best-practices/cluster-large/",children:"Best practices for large clusters"}),"."]}),"\n",(0,i.jsx)(t.p,{children:"SCS wishes to prefer distributed, highly-available systems due to their obvious advantages\nlike fault-tolerance and data redundancy. But it also understands the costs and overhead\nfor the providers associated with this effort, since the infrastructure needs to have\nhardware which will just be used to provide fail-over safety or duplication."}),"\n",(0,i.jsxs)(t.p,{children:["The document ",(0,i.jsx)(t.a,{href:"https://kubernetes.io/docs/setup/best-practices/cluster-large/",children:"Best practices for large clusters"})," describes the concept of a failure zone.\nThis term isn't defined any further, but can in this context be described as a number of\nphysical (computing) machines in such a vicinity to each other (either through physical\nor logical interconnection in some way), that specific problems inside this zone would put\nall these machines at risk of failure/shutdown. It is therefore necessary for important\ndata or services to not be present just on one failure zone.\nHow such a failure zone should be defined is dependent on the risk model of the service/data\nand its owner as well as the capabilities of the provider. Zones could be set from things\nlike single machines or racks up to whole datacenters or even regions, which could be\ncoupled by things like electrical grids. They're therefore purely logical entities, which\nshouldn't be defined further in this document."]}),"\n",(0,i.jsx)(t.h2,{id:"decision",children:"Decision"}),"\n",(0,i.jsx)(t.p,{children:"This standard formulates the requirement for the distribution of Kubernetes nodes in order\nto provide a fault-tolerant and available Kubernetes cluster infrastructure."}),"\n",(0,i.jsxs)(t.p,{children:["The control plane nodes MUST be distributed over multiple physical machines.\nKubernetes provides ",(0,i.jsx)(t.a,{href:"https://kubernetes.io/docs/setup/best-practices/multiple-zones/",children:"best-practices"})," on this topic, which are also RECOMMENDED by SCS."]}),"\n",(0,i.jsx)(t.p,{children:'At least one control plane instance MUST be run in each "failure zone" used for the cluster,\nmore instances per "failure zone" are possible to provide fault-tolerance inside a zone.'}),"\n",(0,i.jsx)(t.p,{children:'Worker nodes are RECOMMENDED to be distributed over multiple zones. This policy makes\nit OPTIONAL to provide a worker node in each "failure zone", meaning that worker nodes\ncan also be scaled vertically first before scaling horizontally.'}),"\n",(0,i.jsx)(t.p,{children:"To provide metadata about the node distribution and possibly provide the ability\nto schedule workloads efficiently, which also enables testing of this standard,\nproviders MUST annotate their K8s nodes with the labels listed below.\nThese labels MUST be kept up to date with the current state of the deployment."}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.code,{children:"topology.kubernetes.io/zone"})}),"\n",(0,i.jsxs)(t.p,{children:["Corresponds with the label described in ",(0,i.jsx)(t.a,{href:"https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone",children:"K8s labels documentation"}),".\nIt provides a logical zone of failure on the side of the provider, e.g. a server rack\nin the same electrical circuit or multiple machines bound to the internet through a\nsingular network structure. How this is defined exactly is up to the plans of the provider.\nThe field gets autopopulated most of the time by either the kubelet or external mechanisms\nlike the cloud controller."]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.code,{children:"topology.kubernetes.io/region"})}),"\n",(0,i.jsxs)(t.p,{children:["Corresponds with the label described in ",(0,i.jsx)(t.a,{href:"https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone",children:"K8s labels documentation"}),".\nIt describes the combination of one or more failure zones into a region or domain, therefore\nshowing a larger entity of logical failure zone. An example for this could be a building\ncontaining racks that are put into such a zone, since they're all prone to failure, if e.g.\nthe power for the building is cut. How this is defined exactly is also up to the provider.\nThe field gets autopopulated most of the time by either the kubelet or external mechanisms\nlike the cloud controller."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"previous-standard-versions",children:"Previous standard versions"}),"\n",(0,i.jsxs)(t.p,{children:["This is version 2 of the standard; it extends ",(0,i.jsx)(t.a,{href:"/standards/scs-0214-v1-k8s-node-distribution",children:"version 1"})," with the\nrequirements regarding node labeling."]})]})}function h(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>a});var s=n(96540);const i={},o=s.createContext(i);function r(e){const t=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(o.Provider,{value:t},e.children)}}}]);