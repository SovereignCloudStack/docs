<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-iaas/components/openstack-health-monitor" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Setting up OpenStack health monitor on Debian | One platform — standardized, built and operated by many.</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docs.scs.community/img/scs-og-basic.png"><meta data-rh="true" name="twitter:image" content="https://docs.scs.community/img/scs-og-basic.png"><meta data-rh="true" property="og:url" content="https://docs.scs.community/docs/iaas/components/openstack-health-monitor"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Setting up OpenStack health monitor on Debian | One platform — standardized, built and operated by many."><meta data-rh="true" name="description" content="Kurt Garloff, 2024-02-20"><meta data-rh="true" property="og:description" content="Kurt Garloff, 2024-02-20"><link data-rh="true" rel="icon" href="/img/favicon.png"><link data-rh="true" rel="canonical" href="https://docs.scs.community/docs/iaas/components/openstack-health-monitor"><link data-rh="true" rel="alternate" href="https://docs.scs.community/docs/iaas/components/openstack-health-monitor" hreflang="en"><link data-rh="true" rel="alternate" href="https://docs.scs.community/docs/iaas/components/openstack-health-monitor" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="SCS Community Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="SCS Community Blog Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="SCS Community Blog JSON Feed">









<link rel="preconnect" href="https://matomo.scs.community/">
<script>var _paq=window._paq=window._paq||[];_paq.push(["setDocumentTitle",document.domain+"/"+document.title]),_paq.push(["disableCookies"]),_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){var e="https://matomo.scs.community/";_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","2"]);var a=document,t=a.createElement("script"),p=a.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",p.parentNode.insertBefore(t,p)}()</script><link rel="stylesheet" href="/assets/css/styles.7136250c.css">
<script src="/assets/js/runtime~main.ff722ccb.js" defer="defer"></script>
<script src="/assets/js/main.6cc1e574.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="SCS" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="SCS" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/standards">Standards</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs">For Operators</a><a class="navbar__item navbar__link" href="/contributor-docs">For Contributors</a><a class="navbar__item navbar__link" href="/user-docs">For Users</a><a class="navbar__item navbar__link" href="/community">Community</a><a class="navbar__item navbar__link" href="/docs/faq">FAQ</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/SovereignCloudStack/docs" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Setting up OpenStack health monitor on Debian</h1></header>
<p>Kurt Garloff, 2024-02-20</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="intro">Intro<a href="#intro" class="hash-link" aria-label="Direct link to Intro" title="Direct link to Intro" translate="no">​</a></h2>
<p>The development of <a href="https://github.com/SovereignCloudStack/openstack-health-monitor/" target="_blank" rel="noopener noreferrer" class="">openstack-health-monitor</a> was done on <a href="https://kfg.images.obs-website.eu-de.otc.t-systems.com/" target="_blank" rel="noopener noreferrer" class="">openSUSE 15.x images</a>, just because the author is very familiar with it and has some of the needed tools preinstalled. That said, the setup is not depending on anything specific from openSUSE and should work on every modern Linux distribution.</p>
<p>Setting it up again in a different environment using Debian 12 images avoids a few of the shortcuts that were used and thus should be very suitable instructions to get it working in general. The step by step instructions are covered here.</p>
<p>Note: This is a rather classical snowflake setup -- we create a VM and do some manual configuration to get everything configured. Having it well documented here should make this more replicatable, and is an important precondition for more automation, but larger steps to full automate this using ansible or helm charts (in a containerized variant) are not addressed here. As we expect a <a href="https://github.com/SovereignCloudStack/scs-health-monitor" target="_blank" rel="noopener noreferrer" class="">successor project</a> for the increasingly hard to maintain shell code, this may not be worth the trouble.</p>
<p>openstack-health-monitor implements a scripted scenario test with a large shell-script that uses the openstackclient tools to set up the scenario, test it and tear everything down again in a loop. Any errors are recorded, as well as timings and some very basic benchmarks. The script sets up some virtual network infrastructure (routers, networks, subnets, floating IPs), security groups, keypairs, volumes and finally boots some VMs. Access to these is tested (ensuring metadata injection works) and connectivity between them tested and measured. A loadbalancer (optionally) is set up with a health-monitor and access via it before and after killing some backends is tested.
The scenario is described in a bit more detail in the <a href="https://github.com/SovereignCloudStack/openstack-health-monitor/blob/main/README.md" target="_blank" rel="noopener noreferrer" class="">repository&#x27;s README.md</a> file.</p>
<p>The openstack-health-monitor is not the intended long-term solution for monitoring your infrastructure. The SCS project has a project underway that will create more modern, flexible, and more maintainable monitoring infrastructure; the concepts are described on the <a href="https://docs.scs.community/docs/category/monitoring" target="_blank" rel="noopener noreferrer" class="">monitoring section</a> of the project&#x27;s documentation. The openstack-health-monitor will thus not see any significant enhancements any more; it will be maintained and kept alive as long as there are users. This guide exclusively focuses on how to set it up.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="setting-up-the-driver-vm">Setting up the driver VM<a href="#setting-up-the-driver-vm" class="hash-link" aria-label="Direct link to Setting up the driver VM" title="Direct link to Setting up the driver VM" translate="no">​</a></h2>
<p>So we start a <code>Debian 12</code> image on a cloud of our choice. This should work on any OpenStack cloud that is reasonably standard;
the instructions use flavor names and image names from the SCS standards.
For many, the simplest way may be to use the Web-UI of their cloud (e.g. horizon for OpenStack).</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="internal-vs-external-monitoring">Internal vs external monitoring<a href="#internal-vs-external-monitoring" class="hash-link" aria-label="Direct link to Internal vs external monitoring" title="Direct link to Internal vs external monitoring" translate="no">​</a></h3>
<p>There are pros and cons to run the driver VM in the same cloud that is also under test. We obviously don&#x27;t test the external reachability of the cloud (more precisely its API endpoints and VMs) if we run it on the same cloud -- which may or may not be desirable. Having the tests happily continuing to collect data  may actually be valuable in times when external access is barred. If the cloud goes down, we will no longer see API calls against it, although the information of them not being available does not reveal much in terms of insight into the reasons for the outage. Also, the driver VM is the only long-lived VM in the openstack-health-monitor setup, so it may be useful to have it in the same cloud to reveal any issues that do not occur on the short-lived resources created and deleted by the health-monitor.</p>
<p>The author tends to see running it internally as advantageous -- ideally combined with a simple API reachability test from the outside that sends alarms as needed to detect any reachability problems.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="unprivileged-operation">Unprivileged operation<a href="#unprivileged-operation" class="hash-link" aria-label="Direct link to Unprivileged operation" title="Direct link to Unprivileged operation" translate="no">​</a></h3>
<p>Nothing in this test requires admin privileges on the cloud where the driver runs nor on the cloud under test. We do install and configure a few software packages in the driver VM, which requires sudo power there, but the script should just run as a normal user. For the cloud under test it is recommended to use a user (or an application credential) with a normal tenant member role to access the cloud under test. If you can, give it an OpenStack project on its own.</p>
<p>If <code>openstack availability zone list --compute</code> fails for you without admin rights, please fix your openstack client, e.g. by applying the <a href="https://raw.githubusercontent.com/SovereignCloudStack/openstack-health-monitor/main/docs/openstackclient-az-list-fallback-f3207bd.diff" target="_blank" rel="noopener noreferrer" class="">patch</a> I mentioned in <a href="https://storyboard.openstack.org/#!/story/2010989" target="_blank" rel="noopener noreferrer" class="">this issue</a>. (Versions 6.3.0 and 6.4.0 are broken.) Do not consider giving the OpenStack Health-Monitor admin power. (Note: It has a workaround for the broken AZ listing using curl now.)</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="driver-vm-via-openstack-cli">Driver VM via openstack CLI<a href="#driver-vm-via-openstack-cli" class="hash-link" aria-label="Direct link to Driver VM via openstack CLI" title="Direct link to Driver VM via openstack CLI" translate="no">​</a></h3>
<p>The author prefers to setup the VM via <code>openstack</code> CLI tooling. He has working entries for all clouds he uses in his <code>~/.config/openstack/clouds.yaml</code> and <code>secure.yaml</code> and has exported the <code>OS_CLOUD</code> environment variable to point to the cloud he is working on to set up the driver VM. The author uses the <code>bash</code> shell. All of this of course could be scripted.</p>
<p>So here we go</p>
<ol>
<li class="">Create the network setup for a VM in a network <code>oshm-network</code> with an IPv4 subnet, connected to a router that connects (and by default SNATs) to the public network.</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">PUBLIC=$(openstack network list --external -f value -c Name)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack router create oshm-router</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack router set --external-gateway $PUBLIC oshm-driver-router</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack network create oshm-network</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack subnet create --subnet-range 192.168.192.0/24 --network oshm-network oshm-subnet</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack router add subnet oshm-router oshm-subnet</span><br></span></code></pre></div></div>
<ol start="2">
<li class="">Create a security group that allows ssh and ping access</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack security group create sshping</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack security group rule create --ingress --ethertype ipv4 --protocol tcp --dst-port 22 sshping</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack security group rule create --ingress --ethertype ipv4 --protocol icmp --icmp-type 8 sshping</span><br></span></code></pre></div></div>
<ol start="3">
<li class="">Being at it, we also create the security group for grafana</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack security group create grafana</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack security group rule create --ingress --ethertype ipv4 --protocol tcp --dst-port 3000 grafana</span><br></span></code></pre></div></div>
<ol start="4">
<li class="">To connect to the VM via ssh later, we create an SSH keypair</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack keypair create --private-key ~/.ssh/oshm-key.pem oshm-key</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">chmod og-r ~/.ssh/oshm-key.pem</span><br></span></code></pre></div></div>
<p>Rather than creating a new key (and storing and protecting the private key), we could have passed <code>--public-key</code> and used an existing keypair.</p>
<ol start="5">
<li class="">Look up Debian 12 image UUID.</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">IMGUUID=$(openstack image list --name &quot;Debian 12&quot; -f value -c ID | tr -d &#x27;\r&#x27;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">echo $IMGUUID</span><br></span></code></pre></div></div>
<p>Sidenote: The <code>tr</code> command is there to handle broken tooling that embeds a trailing <code>\r</code> in the output.</p>
<ol start="6">
<li class="">Boot the driver VM</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack server create --network oshm-network --key-name oshm-key --security-group default --security-group sshping --security-group grafana --flavor SCS-2V-4 --block-device boot_index=0,uuid=$IMGUUID,source_type=image,volume_size=10,destination_type=volume,delete_on_termination=true oshm-driver</span><br></span></code></pre></div></div>
<p>Chose a flavor that exists on your cloud. Here we have used  one without root disk and asked nova to create a volume on the fly by passing <code>--block-device</code>. See <a href="https://scs.community/2023/08/21/diskless-flavors/" target="_blank" rel="noopener noreferrer" class="">diskless flavor blog article</a>. For flavors with local root disks, you could have used the <code>--image $IMGUUID</code> parameter instead.</p>
<ol start="7">
<li class="">Wait for it to boot (optional)
You can look at the boot log with <code>openstack console log show oshm-driver</code> or connect to it via VNC at the URL given by <code>openstack console url show oshm-driver</code>. You can of course also query openstack on the status <code>openstack server list</code> or <code>openstack server show oshm-driver</code>. You can also just create a simple loop:</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">declare -i ctr=0 RC=0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">while [ $ctr -le 120 ]; do</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  STATUS=&quot;$(openstack server list --name oshm-driver -f value -c Status)&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  if [ &quot;$STATUS&quot; = &quot;ACTIVE&quot; ]; then echo &quot;$STATUS&quot;; break; fi</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  if [ &quot;$STATUS&quot; = &quot;ERROR&quot; ]; then echo &quot;$STATUS&quot;; RC=1; break; fi</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  if [ -z &quot;$STATUS&quot; ]; then echo &quot;No such VM&quot;; RC=2; break; fi</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  sleep 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  let ctr+=1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">done</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># return $RC</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">if [ $RC != 0 ]; then false; fi</span><br></span></code></pre></div></div>
<ol start="8">
<li class="">Attach a floating IP so it&#x27;s reachable from the outside.</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">FIXEDIP=$(openstack server list --name oshm-driver -f value -c Networks |  sed &quot;s@^[^:]*:[^&#x27;]*&#x27;\([0-9\.]*\)&#x27;.*\$@\1@&quot;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">FIXEDPORT=$(openstack port list --fixed-ip ip-address=$FIXEDIP,subnet=oshm-subnet -f value -c ID)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">echo $FIXEDIP $FIXEDPORT</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack floating ip create --port $FIXEDPORT $PUBLIC</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">FLOATINGIP=$(openstack floating ip list --fixed-ip-address $FIXEDIP -f value -c &quot;Floating IP Address&quot;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">echo &quot;Floating IP: $FLOATINGIP&quot;</span><br></span></code></pre></div></div>
<p>Remember this floating IP address.</p>
<ol start="9">
<li class="">Connect to it via ssh</li>
</ol>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">ssh -i ~/.ssh/oshm-key.pem debian@$FLOATINGIP</span><br></span></code></pre></div></div>
<p>On the first connection, you need to accept the new ssh host key. (Very careful people would compare the fingerprint with the console log output.)</p>
<p><strong>All the following commands are performed on the newly started driver VM.</strong></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="configuring-openstack-cli-on-the-driver-vm">Configuring openstack CLI on the driver VM<a href="#configuring-openstack-cli-on-the-driver-vm" class="hash-link" aria-label="Direct link to Configuring openstack CLI on the driver VM" title="Direct link to Configuring openstack CLI on the driver VM" translate="no">​</a></h3>
<p>We need to install the openstack client utilities.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo apt-get update</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo apt-get install python3-openstackclient</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo apt-get install python3-cinderclient python3-octaviaclient python3-swiftclient python3-designateclient</span><br></span></code></pre></div></div>
<p>Configure your cloud access in <code>~/.config/openstack/clouds.yaml</code></p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token key atrule">clouds</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">CLOUDNAME</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">interface</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> public</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">identity-api-version</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token number">3</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)">#region_name: REGION</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">auth</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">auth_url</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> KEYSTONE_ENDPOINT</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">project_id</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> PROJECT_UUID</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token comment" style="color:rgb(98, 114, 164)">#alternatively project_name and project_domain_name</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">user_domain_name</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> default</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token comment" style="color:rgb(98, 114, 164)"># change to your real domain</span><br></span></code></pre></div></div>
<p>and <code>secure.yaml</code> (in the same directory)</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token key atrule">clouds</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">CLOUDNAME</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">auth</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">username</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> USERNAME</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">password</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> PASSWORD</span><br></span></code></pre></div></div>
<p>The <code>CLOUDNAME</code> can be freely chosen. This is the value passed to the openstack CLI with <code>--os-cloud</code> or exported to your environment in <code>OS_CLOUD</code>. The other uppercase words need to be adjusted to match your cloud. Hint: horizon typically lets you download a sample <code>clouds.yaml</code> file that works (but lacks the password).</p>
<p>Protect your <code>secure.yaml</code> from being read by others: <code>chmod 0600 ~/.config/openstack/secure.yaml</code>.</p>
<p>If you are using application credentials instead of username, password to authenticate, you don&#x27;t need to specify <code>project_id</code> nor project&#x27;s nor user&#x27;s domain names in <code>clouds.yaml</code>. Just (in <code>secure.yaml</code>):</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token key atrule">clouds</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">CLOUDNAME</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">auth_type</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> v3applicationcredential</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">auth</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">application_credential_id</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> APPCRED_ID</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      </span><span class="token key atrule">application_credential_secret</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;APPCRED_SECRET&quot;</span><br></span></code></pre></div></div>
<p>Configure this to be your default cloud:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">export OS_CLOUD=CLOUDNAME</span><br></span></code></pre></div></div>
<p>You might consider adding this to your <code>~/.bashrc</code> for convenience. Being at it, you might want to add <code>export CLIFF_FIT_WIDTH=1</code> there as well to make openstack command output tables more readable (but sometimes less easy to cut&#x27;n&#x27;paste).</p>
<p>Verify that your openstack CLI works:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack catalog list</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack server list</span><br></span></code></pre></div></div>
<p>You can use the same project as you use for your driver VM (and possibly other workloads). The openstack-health-monitor is carefully designed to not clean up anything that it has not created. There is however some trickiness, as not all resources have names (floating IPs for example do not) and sometimes names need to be assigned after creation of a resource (volumes of diskless flavors), so in case there are API errors, some heuristics is used to identify resources which may not be safe under all circumstances. So ideally, you have an extra project created just for the health-monitor and configure the credentials for it here, so you can not possibly hit any wrong resource in the script&#x27;s extensive efforts to clean up in error cases.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="custom-ca">Custom CA<a href="#custom-ca" class="hash-link" aria-label="Direct link to Custom CA" title="Direct link to Custom CA" translate="no">​</a></h3>
<p>If your cloud API&#x27;s endpoints don&#x27;t use TLS certificates that are signed by an official CA, you need to provide your CA to this VM and configure it. (On a SCS Cloud-in-a-Box system, you find it on the manager node in <code>/etc/ssl/certs/ca-certificates.crt</code>. You may extract the last cert or just leave them all together.) Copy the CA file to your driver VM and ensure it&#x27;s readable by the <code>debian</code> user.</p>
<p>Add it to your <code>clouds.yaml</code></p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token key atrule">clouds</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  </span><span class="token key atrule">CLOUDNAME</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token key atrule">cacert</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"> /PATH/TO/CACERT.CRT</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token punctuation" style="color:rgb(248, 248, 242)">...</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><br></span></code></pre></div></div>
<p>If you want to allow <code>api_monitor.sh</code> to be able to talk to the service endpoints directly to avoid getting a fresh token from keystone for each call, you also need to export it to your environment:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">export OS_CACERT=/PATH/TO/CACERT.CRT</span><br></span></code></pre></div></div>
<p>Consider adding this to your <code>~/.bashrc</code> as well.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="your-first-api_monitorsh-iteration">Your first <code>api_monitor.sh</code> iteration<a href="#your-first-api_monitorsh-iteration" class="hash-link" aria-label="Direct link to your-first-api_monitorsh-iteration" title="Direct link to your-first-api_monitorsh-iteration" translate="no">​</a></h2>
<p>Checkout openstack-health-monitor:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo apt-get install git bc jq netcat-traditional tmux zstd</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">git clone https://github.com/SovereignCloudStack/openstack-health-monitor</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd openstack-health-monitor</span><br></span></code></pre></div></div>
<p>You may want to start a <code>tmux</code> (or <code>screen</code>) session now, so you can do multiple things in parallel (e.g. for debugging) and reconnect.</p>
<p>The script <code>api_monitor.sh</code> is the main worker of openstack-health-monitor and runs one to many iterations of a cycle where resources are created, tested and torn down. Its operation is described in the <a href="https://github.com/SovereignCloudStack/openstack-health-monitor/blob/main/README.md" target="_blank" rel="noopener noreferrer" class="">README.md</a> file.</p>
<p>It is good practice to use <code>tmux</code>. This allows you to return (reattach) to console sessions and to open new windows to investigate things. Traditional people may prefer to <code>screen</code> over <code>tmux</code>.</p>
<p>You should be ready to run one iteration of the openstack-health-monitor now. Run it like this:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">export IMG=&quot;Debian 12&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export JHIMG=&quot;Debian 12&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">./api_monitor.sh -O -C -D -n 6 -s -b -B -M -T -LL -i 1</span><br></span></code></pre></div></div>
<p>Leave out the <code>-LL</code> if you don&#x27;t have a working loadbalancer service or replace <code>-LL</code> with <code>-LO</code> if you want to test the ovn loadbalancer instead of amphorae (saving quite some resources).</p>
<p>Feel free to study the meaning of all the command line parameters by looking at the <a href="https://github.com/SovereignCloudStack/openstack-health-monitor/blob/main/README.md" target="_blank" rel="noopener noreferrer" class="">README.md</a>. (Note: Many of the things enabled by the parameters should be default, but are not for historic reasons. This would change if we rewrite this whole thing in python.)</p>
<p>This will run for ~7 minutes, depending on the performance of your OpenStack environment. You should not get any error. (The amber-colored outputs <code>DOWN</code>, <code>BUILD</code>, <code>creating</code> are not errors. Nothing in red should be displayed.) Studying the console output may be instructive to follow the script&#x27;s progress. You may also open another window (remember the tmux recommendation above) and look at the resources with the usual <code>openstack RESOURCE list</code> and <code>openstack RESOURCE show NAME</code> and <code>RESOURCE</code> being something like <code>router</code>, <code>network</code>, <code>subnet</code>, <code>port</code>, <code>volume</code>, <code>server</code>, <code>floating ip</code>, <code>loadbalancer</code>, <code>loadbalancer pool</code>, <code>loadbalancer listener</code>, <code>security group</code>, <code>keypair</code>, <code>image</code>, ...)</p>
<p>The <code>api_monitor.sh</code> uses and <code>APIMonitor_TIMESTAMP</code> prefix for all OpenStack resource names. This allows to identify the created resources and clean them up even if things go wrong.
<code>TIMESTAMP</code> is an integer number representing the seconds after 1970-01-01 00:00:00 UTC (Unix time).</p>
<p>This may be the time to check that you have sufficient quota to create the resources. While we only create 6+N VMs (and volumes) with the above call (N being the number of AZs), we would want to increase this number for larger clouds. For single-AZ deployments, we would want to still use 2 networks at least <code>-N 2</code> to test the ability of the router to route traffic between networks. So expect <code>-n 6</code> to become <code>-N 2 -n 6</code> for a very small single-AZ cloud or <code>-n 12</code> for a large 3 AZ cloud region. So, re-run the <code>api_monitor.sh</code> with the target sizing.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="resource-impact-and-charging">Resource impact and charging<a href="#resource-impact-and-charging" class="hash-link" aria-label="Direct link to Resource impact and charging" title="Direct link to Resource impact and charging" translate="no">​</a></h3>
<p>Note that <code>api_monitor.sh</code> uses small flavors (<code>SCS-1V-2</code> for the N jump hosts and <code>SCS-1L-1</code> for the other VMs) to keep the impact on your cloud (and on your invoice if you are not monitoring your own cloud) small. You can change the flavors.</p>
<p>If you have to pay for this, also consider that some clouds are not charging by the minute but may count by the started hour. So when you run <code>api_monitor.sh</code> in a loop (which you will) with say 10 VMs (e.g. <code>-N 2 -n 8</code>) in each iteration and run this for an hour with 8 iterations, you will never have more than 10 VMs in parallel and they only are alive a bit more than half of the time, but rather than being charged for ~6 VM hours, you end up being charged for ~80 VM hours. Similar for volumes, routers, floating IPs. This makes a huge difference.</p>
<p>Sometimes the cloud under test has issues. That&#x27;s why we do monitoring ... One thing that might happen is that loadbalancers and volumes (and other resources, but those two are the most prone to this) end up in a broken state that can not be cleaned up by the user any more. Bad providers may charge for these anyhow, although this will never stand a legal dispute. (IANAL, but charging for providing something that is not working is not typically supported by civil law in most jurisdictions and T&amp;Cs that would say so would not normally be legally enforceable.) If this happens, I recommend to keep records of the broken state (store the output of <code>openstack volume list</code>, <code>openstack volume show BROKEN_VOLUME</code>, <code>openstack loadbalancer list</code>, <code>openstack loadbalancer show BROKEN_LB</code>.)</p>
<p>Using <code>-w -1</code> makes <code>api_monitor.sh</code> wait for interactive input whenever an error occurs; this can be convenient for debugging.</p>
<p>Once you have single iterations working nicely, we can proceed.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="automating-startup-and-cleanup">Automating startup and cleanup<a href="#automating-startup-and-cleanup" class="hash-link" aria-label="Direct link to Automating startup and cleanup" title="Direct link to Automating startup and cleanup" translate="no">​</a></h2>
<p>Typically, we run <code>api_monitor.sh</code> with a limited amount of iterations (200) and then restart it. For each restart, we also output some statistics, compress the log file and look at any leftovers that did not get cleaned up. The latter happens in the start script that we create here.</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">#!/bin/bash</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># run_CLOUDNAME.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Do some global settings</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export IMG=&quot;Debian 12&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export JHIMG=&quot;Debian 12&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#export OS_CACERT=/home/debian/ca-certificates.pem</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Additional settings to override flavors or to</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># configure email addresses for sending alarms can be set here</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Does openstack CLI work?</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack server list &gt;/dev/null || exit 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Upload log files to this swift container (which you need to create)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#export SWIFTCONTAINER=OS-HM-Logfiles</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># CLEANUP</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">echo &quot;Finding resources from previous runs to clean up ...&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Find Floating IPs</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">FIPLIST=&quot;&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">FIPS=$(openstack floating ip list -f value -c ID)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">for fip in $FIPS; do</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        FIP=$(openstack floating ip show $fip | grep -o &quot;APIMonitor_[0-9]*&quot;)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        if test -n &quot;$FIP&quot;; then FIPLIST=&quot;${FIPLIST}${FIP}_</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&quot;; fi</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">done</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">FIPLIST=$(echo &quot;$FIPLIST&quot; | grep -v &#x27;^$&#x27; | sort -u)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Cleanup previous interrupted runs</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">SERVERS=$(openstack server  list | grep -o &quot;APIMonitor_[0-9]*_&quot; | sort -u)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">KEYPAIR=$(openstack keypair list | grep -o &quot;APIMonitor_[0-9]*_&quot; | sort -u)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">VOLUMES=$(openstack volume  list | grep -o &quot;APIMonitor_[0-9]*_&quot; | sort -u)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">NETWORK=$(openstack network list | grep -o &quot;APIMonitor_[0-9]*_&quot; | sort -u)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">LOADBAL=$(openstack loadbalancer list | grep -o &quot;APIMonitor_[0-9]*_&quot; | sort -u)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ROUTERS=$(openstack router  list | grep -o &quot;APIMonitor_[0-9]*_&quot; | sort -u)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">SECGRPS=$(openstack security group list | grep -o &quot;APIMonitor_[0-9]*_&quot; | sort -u)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">echo CLEANUP: FIPs $FIPLIST Servers $SERVERS Keypairs $KEYPAIR Volumes $VOLUMES Networks $NETWORK LoadBalancers $LOADBAL Routers $ROUTERS SecGrps $SECGRPS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">for ENV in $FIPLIST; do</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  echo &quot;******************************&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  echo &quot;CLEAN $ENV&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  bash ./api_monitor.sh -o -T -q -c CLEANUP $ENV</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  echo &quot;******************************&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">done</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">TOCLEAN=$(echo &quot;$SERVERS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$KEYPAIR</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$VOLUMES</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$NETWORK</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$LOADBAL</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$ROUTERS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">$SECGRPS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">&quot; | grep -v &#x27;^$&#x27; | sort -u)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">for ENV in $TOCLEAN; do</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  echo &quot;******************************&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  echo &quot;CLEAN $ENV&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  bash ./api_monitor.sh -o -q -LL -c CLEANUP $ENV</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  echo &quot;******************************&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">done</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Now run the monitor</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#exec ./api_monitor.sh -O -C -D -N 2 -n 6 -s -M -LO -b -B -a 2 -t -T -R -S ciab &quot;$@&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">exec ./api_monitor.sh -O -C -D -N 2 -n 6 -s -M -LO -b -B -T &quot;$@&quot;</span><br></span></code></pre></div></div>
<p>Compared to the previous run, we have explicitly set two networks here <code>-N 2</code> and rely on the iterations being passed in as command line arguments. Add parameter <code>-t</code> if your cloud is slow to increase timeouts. We have enabled the octavia loadbalancer (<code>-LO</code>) in this example rather than the amphora based one (<code>-LL</code>).</p>
<p>You may use one of the existing <code>run_XXXX.sh</code> scripts as example. Beware: eMail alerting with <code>ALARM_EMAIL_ADDRESS</code> and <code>NOTE_EMAIL_ADDRESS</code> (and limiting with <code>-a</code> and <code>-R</code> ) and reporting data to telegraf (option <code>-S</code>) may be present in the samples. Make this script executable (<code>chmod +x run_CLOUDNAME.sh</code>).</p>
<p>We wrap a loop around this in <code>run_in_loop.sh</code>:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">#!/bin/bash</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># run_in_loop.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">rm stop-os-hm 2&gt;/dev/null</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">while true; do</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  ./run_CLOUDNAME.sh -i 200</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  if test -e stop-os-hm; then break; fi</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  echo -n &quot;Hit ^C to abort ...&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  sleep 15; echo</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">done</span><br></span></code></pre></div></div>
<p>Also make this executable (<code>chmod +x run_in_loop.sh</code>).
To run this automatically in a tmux window whenever the system starts, we follow the steps in the <a href="https://github.com/SovereignCloudStack/openstack-health-monitor/blob/main/startup/README.md" target="_blank" rel="noopener noreferrer" class="">startup README.md</a></p>
<p>Change <code>OS_CLOUD</code> in <code>startup/run-apimon-in-tmux.sh</code>. (If you need to set <code>OS_CACERT</code>, also add it in this file and pass it into the windows.)</p>
<p>Activate everything:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">mkdir -p ~/.config/systemd/user/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cp -p startup/apimon.service ~/.config/systemd/user/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">systemctl --user enable apimon</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">systemctl --user start apimon</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo loginctl enable-linger debian</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tmux attach -t oshealthmon</span><br></span></code></pre></div></div>
<p>This assumes that you are using the user <code>debian</code> for this monitoring and have checked out the repository at <code>~/openstack-health-monitor/</code>. Adjust the paths and user name otherwise. (If for whatever reason you have chosen to install things as root, you will have to install the systemd service unit in the system paths and ensure it&#x27;s not started too early in the boot process.)</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="changing-parameters-and-restarting">Changing parameters and restarting<a href="#changing-parameters-and-restarting" class="hash-link" aria-label="Direct link to Changing parameters and restarting" title="Direct link to Changing parameters and restarting" translate="no">​</a></h3>
<p>If you want to change the parameters passed to <code>api_monitor.sh</code>, you best do this by editing <code>run_CLOUDNAME.sh</code>, potentially after testing it with one iteration before.</p>
<p>To make the change effective, you can wait until the current 200 iterations are completed and the <code>run_in_loop.sh</code> calls <code>run_CLOUDNAME.sh</code> again. You can also hit <code>^C</code> in the tmux window that has<code>api_monitor.sh</code> running. The script will then exit after the current iteration. Note that sending this interrupt is handled by the script, so it does still continue the current iteration and do all the cleanup work. However, you may interrupt an API call and thus cause a spurious error (which may in the worst case lead to a couple more spurious errors). If you want to avoid this, hit <code>^C</code> during the wait/sleep phases of the script (after having done all the tests or after having completed the iteration). If you hit <code>^C</code> twice, it will abort the the current iteration, but still try to clean up. Then the outer script will also exit and you have to restart by manually calling <code>./run_in_loop.sh</code> again.</p>
<p>You can also issue the <code>systemctl --user stop apimon</code> command; it will basically do the same thing: Send <code>^C</code> and then wait for everything to be completed and tear down the tmux session.
After waiting for that to complete, you can start it again with <code>systemctl --user start apimon</code>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="multiple-instances">Multiple instances<a href="#multiple-instances" class="hash-link" aria-label="Direct link to Multiple instances" title="Direct link to Multiple instances" translate="no">​</a></h3>
<p>You can run multiple instances of <code>api_monitor.sh</code> on the same driver VM. In this case, you should rename <code>run_in_loop.sh</code> to e.g. <code>run_in_loop_CLOUDNAME1.sh</code> and call <code>run_CLOUDNAME1.sh</code> from there. Don&#x27;t forget to adjust <code>startup/run-apimon-in-tmux.sh</code> and <code>startup/kill-apimon-in-tmux.sh</code> to start more windows.</p>
<p>It is not recommended to run multiple instances against the same OpenStack project however. While the <code>api_monitor.sh</code> script carefully keeps track of its own resources and avoids to delete things it has not created, this is not the case for the <code>run_CLOUDNAME.sh</code> script, which is explicitly meant to identify anything in the target project that was created by a health monitor and clean it up. If it hits the resources that are currently in use by another health mon instance, this will create spurious errors. This will happen every ~200 iterations, so you could still have some short-term coexistence when you are performing debug operations.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="alarming-and-logs">Alarming and Logs<a href="#alarming-and-logs" class="hash-link" aria-label="Direct link to Alarming and Logs" title="Direct link to Alarming and Logs" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="email">eMail<a href="#email" class="hash-link" aria-label="Direct link to eMail" title="Direct link to eMail" translate="no">​</a></h3>
<p>If wanted, the <code>api_monitor.sh</code> can send statistics and error messages via email, so operator personnel is informed about the state of the monitoring. This email notification service potentially results in many emails; one error may produce several mails. So in case of a systematic problem, expect to receive dozens of mails per hour. This can be reduced a bit using the <code>-a N</code> and <code>-R</code> options. In order to enable sending emails from the driver VM, it needs to have <code>postfix</code> (or another MTA) installed and configured and outgoing connections for eMail need to be allowed. Note that many operators prefer not to use the eMail notifications but rather rely on looking at the dashboards (see further down) regularly.</p>
<p>Once you have configured <code>postfix</code>, you can enable eMail notifications using the option <code>-e</code>. Using it twice allows you to differentiate between notes (statistical summaries) and errors. If you want to send mails to more than one recipient, you can do so by passing <code>ALARM_EMAIL_ADDRESSES</code> and <code>NOTE_EMAIL_ADDRESSES</code> environment variables to <code>api_monitor.sh</code>, e.g. by setting it in the <code>run_CLOUDNAME.sh</code>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="log-files">Log files<a href="#log-files" class="hash-link" aria-label="Direct link to Log files" title="Direct link to Log files" translate="no">​</a></h3>
<p><code>api_monitor.sh</code> writes a log file with the name <code>APIMonitor_TIMESTAMP.log</code>. It contains a bit of information to see the progress of the script; more importantly, it logs every single openstack CLI call along with parameters and results. (<code>TIMESTAMP</code> is the Unix time, i.e. seconds since 1970-01-01 00:00:00 UTC.)</p>
<p>Note that <code>api_monitor.sh</code> does take some care not to expose secrets -- since v1.99, it does also redact issued tokens (which would otherwise give you up to 24hrs of access). But the Log files still may contain moderately sensitive information, so we suggest to not share it with untrusted parties.</p>
<p>The log file is written to the file system. After finishing the 200 iterations, the log file is compressed. If the environment variable <code>SWIFTCONTAINER</code> has been set (in <code>run_CLOUDNAME.sh</code>) when starting <code>api_monitor.sh</code>. the log file will be uploaded to a container with that name if it exists and if the swift object storage service is supported by the cloud. So create the container (a bucket in S3 speak) before if you want to use this: <code>export SWIFTCONTAINER=OSHM_Logs; openstack container create $SWIFTCONTAINER</code></p>
<p>After the 200 iterations, a <code>.psv</code> file (pipe-separated values) is created <code>Stats.STARTTIME-ENDTIME.psv</code> (with times as calendar dates) which contains a bit of statistics on the last 200 iterations. This one will also be uploaded to $SWIFTCONTAINER (if configured).</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="data-collection-and-dashboard">Data collection and dashboard<a href="#data-collection-and-dashboard" class="hash-link" aria-label="Direct link to Data collection and dashboard" title="Direct link to Data collection and dashboard" translate="no">​</a></h2>
<p>See <a href="https://github.com/SovereignCloudStack/openstack-health-monitor/blob/main/dashboard/README.md" target="_blank" rel="noopener noreferrer" class="">https://github.com/SovereignCloudStack/openstack-health-monitor/blob/main/dashboard/README.md</a></p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="telegraf">Telegraf<a href="#telegraf" class="hash-link" aria-label="Direct link to Telegraf" title="Direct link to Telegraf" translate="no">​</a></h3>
<p>To install telegraf on Debian 12, we need to add the apt repository provided by InfluxData:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo curl -fsSL https://repos.influxdata.com/influxdata-archive_compat.key -o /etc/apt/keyrings/influxdata-archive_compat.key</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">echo &quot;deb [signed-by=/etc/apt/keyrings/influxdata-archive_compat.key] https://repos.influxdata.com/debian stable main&quot; | sudo tee /etc/apt/sources.list.d/influxdata.list</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo apt update</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo apt -y install telegraf</span><br></span></code></pre></div></div>
<p>In the config file <code>/etc/telegraf/telegraf.conf</code>, we enable</p>
<div class="language-toml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-toml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[[inputs.influxdb_listener]]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  service_address = &quot;:8186&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">[[outputs.influxdb]]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  urls = [&quot;http://127.0.0.1:8086&quot;]</span><br></span></code></pre></div></div>
<p>and restart the service (<code>sudo systemctl restart telegraf</code>).
Enable it on system startup: <code>sudo systemctl enable telegraf</code>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="influxdb">InfluxDB<a href="#influxdb" class="hash-link" aria-label="Direct link to InfluxDB" title="Direct link to InfluxDB" translate="no">​</a></h3>
<p>We proceed to influxdb:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo apt-get install influxdb</span><br></span></code></pre></div></div>
<p>In the configuration file <code>/etc/influxdb/influxdb.conf</code>, ensure that the http interface on port 8086 is enabled.</p>
<div class="language-toml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-toml codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[http]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  enabled = true</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  bind-address = &quot;:8086&quot;</span><br></span></code></pre></div></div>
<p>Restart influxdb as needed with <code>sudo systemctl restart influxdb</code>.
Also enable it on system startup: <code>sudo systemctl enable influxdb</code>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="add--s-cloudname-to-your-run_cloudnamesh-script">Add <code>-S CLOUDNAME</code> to your <code>run_CLOUDNAME.sh</code> script<a href="#add--s-cloudname-to-your-run_cloudnamesh-script" class="hash-link" aria-label="Direct link to add--s-cloudname-to-your-run_cloudnamesh-script" title="Direct link to add--s-cloudname-to-your-run_cloudnamesh-script" translate="no">​</a></h3>
<p>You need to tell the monitor that it should send data via telegraf to influxdb by adding the parameter <code>-S CLOUDNAME</code> to the <code>api_monitor.sh</code> call in <code>run_CLOUDNAME.sh</code>. Restart it (see above) to make the change effective immediately (and not only after 200 iterations complete).</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="caddy-reverse-proxy">Caddy (Reverse Proxy)<a href="#caddy-reverse-proxy" class="hash-link" aria-label="Direct link to Caddy (Reverse Proxy)" title="Direct link to Caddy (Reverse Proxy)" translate="no">​</a></h3>
<p>We&#x27;re going to deploy Grafana behind <a href="https://caddyserver.com/docs/" target="_blank" rel="noopener noreferrer" class="">Caddy</a> as a reverse proxy.
Caddy is very easy to configure, comes with sensible defaults and can automatically provision TLS
certificates using Let&#x27;s Encrypt.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="install-caddy">Install Caddy<a href="#install-caddy" class="hash-link" aria-label="Direct link to Install Caddy" title="Direct link to Install Caddy" translate="no">​</a></h4>
<p>We follow <a href="https://caddyserver.com/docs/install#debian-ubuntu-raspbian" target="_blank" rel="noopener noreferrer" class="">https://caddyserver.com/docs/install#debian-ubuntu-raspbian</a> to setup the stable APT repository:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo apt install -y debian-keyring debian-archive-keyring apt-transport-https curl</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">curl -1sLf &#x27;https://dl.cloudsmith.io/public/caddy/stable/gpg.key&#x27; | sudo gpg --dearmor -o /usr/share/keyrings/caddy-stable-archive-keyring.gpg</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">curl -1sLf &#x27;https://dl.cloudsmith.io/public/caddy/stable/debian.deb.txt&#x27; | sudo tee /etc/apt/sources.list.d/caddy-stable.list</span><br></span></code></pre></div></div>
<p>And install Caddy:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo apt update</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo apt install caddy</span><br></span></code></pre></div></div>
<p>Ensure it&#x27;s started and starts at boot with <code>sudo systemctl enable --now caddy</code>.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="allow-http-traffic-for-oshm-driver">Allow HTTP traffic for oshm-driver<a href="#allow-http-traffic-for-oshm-driver" class="hash-link" aria-label="Direct link to Allow HTTP traffic for oshm-driver" title="Direct link to Allow HTTP traffic for oshm-driver" translate="no">​</a></h4>
<p>Caddy needs TCP port <code>80</code> opened to be able to process the Let&#x27;s Encrypt HTTP challenge, so let&#x27;s
configure an appropriate security group for <code>oshm-driver</code>:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack security group create http</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack security group rule create --ingress --ethertype ipv4 --protocol tcp --dst-port 80 http</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">openstack server add security group oshm-driver http</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="configure-caddy">Configure Caddy<a href="#configure-caddy" class="hash-link" aria-label="Direct link to Configure Caddy" title="Direct link to Configure Caddy" translate="no">​</a></h4>
<p>Create a file <code>/etc/caddy/Caddyfile</code> with the following contents:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">https://health.YOURCLOUD.osba.sovereignit.cloud:3000 {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    reverse_proxy localhost:3003</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre></div></div>
<p>Replace <code>health.YOURCLOUD.osba.sovereignit.cloud</code> with your actual domain.
You can use a hostname of your liking, but Caddy will create TLS certificates for this host using
the HTTP challenge.
The <code>sovereignit.cloud</code> domain is controlled by the SCS project team and has been used for a number
of health mon instances.</p>
<p>Reload Caddy with <code>sudo systemctl reload caddy</code>. That&#x27;s it.</p>
<p>You should now be able to access <code>https://health.YOURCLOUD.sovereignit.cloud:3000</code> and see a proxy error
page because the Grafana service is not yet running (this is our next step).
The very first request will be a bit slower, because Caddy interacts with Let&#x27;s Encrypt API to create
the TLS certificate behind the scenes.</p>
<p>Caddy logs can be accessed with <code>sudo journalctl -u caddy</code>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="grafana">Grafana<a href="#grafana" class="hash-link" aria-label="Direct link to Grafana" title="Direct link to Grafana" translate="no">​</a></h3>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="install-grafana">Install Grafana<a href="#install-grafana" class="hash-link" aria-label="Direct link to Install Grafana" title="Direct link to Install Grafana" translate="no">​</a></h4>
<p>We follow <a href="https://grafana.com/docs/grafana/latest/setup-grafana/installation/debian/" target="_blank" rel="noopener noreferrer" class="">https://grafana.com/docs/grafana/latest/setup-grafana/installation/debian/</a> and setup the stable APT repository:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">mkdir -p /etc/apt/keyrings</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget -q -O - https://apt.grafana.com/gpg.key | gpg --dearmor | sudo tee /etc/apt/keyrings/grafana.gpg &gt; /dev/null</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">echo &quot;deb [signed-by=/etc/apt/keyrings/grafana.gpg] https://apt.grafana.com stable main&quot; | sudo tee -a /etc/apt/sources.list.d/grafana.list</span><br></span></code></pre></div></div>
<p>And install it:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo apt update</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo apt -y install grafana</span><br></span></code></pre></div></div>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="basic-config">Basic config<a href="#basic-config" class="hash-link" aria-label="Direct link to Basic config" title="Direct link to Basic config" translate="no">​</a></h4>
<p>The config file <code>/etc/grafana/grafana.ini</code> needs some adjustments.</p>
<p>We&#x27;re going to deploy Grafana behind a reverse proxy (Caddy) and configure it as such.</p>
<p>Therefore, in the <code>[server]</code> section:</p>
<div class="language-ini codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-ini codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[server]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">protocol = http</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">http_addr = 127.0.0.1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">http_port = 3003</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">domain = health.YOURCLOUD.sovereignit.cloud</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">root_url = https://%(domain)s:3000/</span><br></span></code></pre></div></div>
<p>Please replace <code>health.YOURCLOUD.sovereignit.cloud</code> with your actual domain.</p>
<p>Next, in the <code>[security]</code> section, set:</p>
<div class="language-ini codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-ini codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[security]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">admin_user = admin</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">admin_password = SOME_SECRET_PASS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">secret_key = SOME_SECRET_KEY</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">data_source_proxy_whitelist = localhost:8088 localhost:8086</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cookie_secure = true</span><br></span></code></pre></div></div>
<p>Please replace <code>SOME_SECRET_PASS</code> and <code>SOME_SECRET_KEY</code> with secure passwords (for example, you can use <code>pwgen -s 20</code>).</p>
<p>Finally, in the <code>[users]</code> section, set:</p>
<div class="language-ini codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-ini codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[users]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">allow_sign_up = false</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">allow_org_create = false</span><br></span></code></pre></div></div>
<p>The configuration file contains secrets and should be protected such that only root and group grafana
can read it:</p>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo chown root:grafana /etc/grafana/grafana.ini</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">sudo chmod 0640 /etc/grafana/grafana.ini</span><br></span></code></pre></div></div>
<p>We do the OIDC connection in the section <code>[auth.github]</code> <a href="#github-oidc-integration" class="">later</a>.</p>
<p>We can now restart the service: <code>sudo systemctl restart grafana-server</code>.
Being at it, also enable it on system startup: <code>sudo systemctl enable grafana-server</code>.</p>
<p>You should now be able to access your dashboard on <code>https://health.YOURCLOUD.sovereignit.cloud:3000</code> and log in
via the configured username <code>admin</code> and your <code>SOME_SECRET_PASS</code> password.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="enable-influx-database-in-grafana">Enable influx database in grafana<a href="#enable-influx-database-in-grafana" class="hash-link" aria-label="Direct link to Enable influx database in grafana" title="Direct link to Enable influx database in grafana" translate="no">​</a></h4>
<p>In the dashboard, go to Home, Connections, choose InfluxDB and Add new datasource. The defaults (database name, InfluxQL query language) work. You need to explicitly set the URL to <code>http://localhost:8086</code> (despite this being the suggestion). Set the database name to <code>telegraf</code>. Save&amp;test should succeed.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="importing-the-dashboard">Importing the dashboard<a href="#importing-the-dashboard" class="hash-link" aria-label="Direct link to Importing the dashboard" title="Direct link to Importing the dashboard" translate="no">​</a></h4>
<p>Go to Home, Dashboards, New, Import.
Upload the dashboard <a href="https://github.com/SovereignCloudStack/openstack-health-monitor/blob/main/dashboard/openstack-health-dashboard.json" target="_blank" rel="noopener noreferrer" class="">.json file</a> from the repository, user the <a href="https://github.com/SovereignCloudStack/openstack-health-monitor/blob/main/dashboard/openstack-health-dashboard-10.json" target="_blank" rel="noopener noreferrer" class="">Grafana-10 variant</a> if you use Grafana 10 or newer.</p>
<p>In the dashboard, go to the settings gear wheel, variables, mycloud and add CLOUDNAME to the list of clouds that can be displayed. (There are some existing SCS clouds in that list.)
Save.</p>
<p>Now choose CLOUDNAME as cloud (top of the dashboard, rightmost dropdown for the mycloud filter variable).</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="no-data-displayed">No data displayed?<a href="#no-data-displayed" class="hash-link" aria-label="Direct link to No data displayed?" title="Direct link to No data displayed?" translate="no">​</a></h4>
<p>Sometimes, you may see a panel displaying &quot;no data&quot; despite the fact that the first full iteration of data has been sent to influx already. This may be a strange interaction between the browser and Grafana -- we have not analyzed whether that is a bug in Grafana.</p>
<p>One way to work around is to go into the setting of the panel (the three dots in the upper right corner), go to edit and start changing one aspect of the query. Apply. Change it back to the original. Apply. The data will appear. Save to be sure it&#x27;s conserved.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="dashboard-features">Dashboard features<a href="#dashboard-features" class="hash-link" aria-label="Direct link to Dashboard features" title="Direct link to Dashboard features" translate="no">​</a></h4>
<p>Look at the top line filters: You can filter to only see certain API calls or certain resources; the graphs are very crowded and filtering to better see what you want to focus on is very well intended.</p>
<p>The first row of panels give a health impression; there are absolute numbers as well as percentage numbers and the panels turn amber and red in case you have too many errors. Note that the colors on the panels with absolute numbers can not take into account whether you look at just a few hours or at weeks. Accordingly, consider the colors a reasonable hint if things are green or not when looking at a ~24 hours interval. This limitation does not affect the colors on the percentage graph, obviously.</p>
<p>You can change the time interval and zoom in also by marking an interval with the mouse. Zooming out to a few months can be a very useful feature to see trends and watch e.g. your API performance, your resource creation times or the benchmarks change over the long term.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="github-oidc-integration">GitHub OIDC Integration<a href="#github-oidc-integration" class="hash-link" aria-label="Direct link to GitHub OIDC Integration" title="Direct link to GitHub OIDC Integration" translate="no">​</a></h4>
<p>The SCS providers do allow all GitHub users that belong to the SovereignCloudStack organization to get Viewer
access to the dashboards.
This allows to exchange experience and to get a feeling for the achievable stability.
(Hint: A single digit number of API call fails per week and no other failures is achievable on loaded clouds.)</p>
<p>OIDC integration is achieved by adjusting the <code>[auth.github]</code> section in <code>/etc/grafana/grafana.ini</code> as follows:</p>
<div class="language-ini codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-ini codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">[auth.github]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">enabled = true</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">client_id = YOUR_CLIENT_ID</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">client_secret = YOUR_CLIENT_SECRET</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">allowed_organizations = [&quot;SovereignCloudStack&quot;]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">role_attribute_path = &quot;&#x27;Viewer&#x27;&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">allow_assign_grafana_admin = false</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">skip_org_role_sync = true</span><br></span></code></pre></div></div>
<p>This config maps all users to the <code>Viewer</code> role regardless of their role in the GitHub Org.
Please replace <code>YOUR_CLIENT_ID</code> and <code>YOUR_CLIENT_SECRET</code> with the OAuth2 credentials that the SCS Org GitHub admins
provided to you.
Finally, don&#x27;t forgot to restart Grafana with <code>sudo systemctl restart grafana-server</code> after adjusting the config.</p>
<p>More information can be found in the <a href="https://grafana.com/docs/grafana/latest/setup-grafana/configure-security/configure-authentication/github/" target="_blank" rel="noopener noreferrer" class="">Grafana documentation for GitHub OAuth2</a>.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="maintenance">Maintenance<a href="#maintenance" class="hash-link" aria-label="Direct link to Maintenance" title="Direct link to Maintenance" translate="no">​</a></h2>
<p>The driver VM is a snowflake: A manually set up system (unless you automate all the above steps, which is possible of course) that holds data and is long-lived. As such it&#x27;s important to be maintained.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="unattended-upgrades">Unattended upgrades<a href="#unattended-upgrades" class="hash-link" aria-label="Direct link to Unattended upgrades" title="Direct link to Unattended upgrades" translate="no">​</a></h3>
<p>It is recommended to ensure maintenance updates are deployed automatically. These are unlikely to negatively impact the openstack-health-monitor. See <a href="https://wiki.debian.org/UnattendedUpgrades" target="_blank" rel="noopener noreferrer" class="">https://wiki.debian.org/UnattendedUpgrades</a>. If you decide against unattended upgrades, it is recommended to install updates manually regularly and especially watch out for issues that affect the services that are exposed to the world: sshd (port 22) and Caddy/Grafana (port 3000).</p>
<p>If you use <code>unattended-upgrades</code>, you should review your settings in <code>/etc/apt/apt.conf.d/50unattended-upgrades</code>,
especially <code>Unattended-Upgrade::Origins-Pattern</code>. It controls which packages are upgraded. If you want Caddy to be
part of the automated updates, add an entry like the following:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">Unattended-Upgrade::Origins-Pattern {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    // ...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;origin=cloudsmith/caddy/stable&quot;;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">};</span><br></span></code></pre></div></div>
<p>(This corresponds to <code>o=cloudsmith/caddy/stable</code> in the output of <code>apt-cache policy</code>).</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="sshd-setup">sshd setup<a href="#sshd-setup" class="hash-link" aria-label="Direct link to sshd setup" title="Direct link to sshd setup" translate="no">​</a></h3>
<p>If you already use SSH keys to sign in to the driver VM, consider setting the following in your <code>/etc/ssh/sshd_config</code>
if not already set:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">PasswordAuthentication no</span><br></span></code></pre></div></div>
<p>Debian&#x27;s <code>openssh-server</code>, by default, is also very open about its version, so you might consider disabling this via:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">DebianBanner no</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="updating-openstack-health-monitor">Updating openstack-health-monitor<a href="#updating-openstack-health-monitor" class="hash-link" aria-label="Direct link to Updating openstack-health-monitor" title="Direct link to Updating openstack-health-monitor" translate="no">​</a></h3>
<p>You can just do a <code>git update</code> in the <code>openstack-health-monitor</code> directory to get the latest improvements. Note that these will only become effective after the 200 iterations have completed. You can speed this up by injecting a <code>^C</code>, see above in the restart section.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="backup">Backup<a href="#backup" class="hash-link" aria-label="Direct link to Backup" title="Direct link to Backup" translate="no">​</a></h3>
<p>The system holds two things that you might consider valuable for long-term storage:
(1) The log files. These are compressed and uploaded to object storage if you enable the <code>SWIFTCONTAINER</code> setting, which probably means that these do not need any additional backing up then.
(2) The influx time series data. Back up the data in <code>/var/lib/influxdb</code>.</p>
<p>Obviously, if you want to recover quickly from a crash, you might consider to also back up telegraf, influx and grafana config files as well as the edited startup scripts, <code>clouds.yaml</code>, etc. Be careful not to expose sensitive data by granting too generous access to your backed up files.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="troubleshooting">Troubleshooting<a href="#troubleshooting" class="hash-link" aria-label="Direct link to Troubleshooting" title="Direct link to Troubleshooting" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="debugging-issues">Debugging issues<a href="#debugging-issues" class="hash-link" aria-label="Direct link to Debugging issues" title="Direct link to Debugging issues" translate="no">​</a></h3>
<p>In case there is trouble with your cloud, the normal course of action to analyze is as follows:</p>
<ul>
<li class="">Look at the dashboard (see above)</li>
<li class="">Connect to the driver VM and attach to the tmux session and look at the console output of <code>api_monitor.sh</code></li>
<li class="">Analyze the logfile (locally on the driver VM or grab it from the object storage)</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="analyzing-failures">Analyzing failures<a href="#analyzing-failures" class="hash-link" aria-label="Direct link to Analyzing failures" title="Direct link to Analyzing failures" translate="no">​</a></h3>
<p>When VM instances are created successfully, but then end up in <code>ERROR</code> state, the <code>api_monitor.sh</code> does an explicit <code>openstack server show</code>, so you will find some details in the tmux session, in the alarm emails (if you use those) and in the log files.</p>
<p>Sometimes the VMs end up being <code>ACTIVE</code> as wanted but then they can&#x27;t be accessed via ssh. More often than not, this is a problem with meta-data service on a compute host. Without metadata, not ssh key is injected and login will fail.</p>
<p>To gather more details, you can look at the console output <code>openstack console log show VM</code> (where <code>VM</code> is the name of the uuid of the affected VM instance). The cloud-init output is often enough to see what has gone wrong. You can log in to the VMs: The jumphosts are directly accessible via <code>ssh -i APIMonitor_XXXXX_JH.pem debian@FIP</code>, whereas the JumpHost does port forwarding to the other VMs that don&#x27;t have their own floating IP address: <code>ssh -i APIMonitor_XXXXX_VM.pem -p 222 debian@FIP</code>. Replace <code>XXXXX</code> with the number in your current APIMonitor prefix, <code>FIP</code> with the floating IP address of the responsible JumpHost and <code>debian</code> with the user name used by the images you boot. Use <code>223</code> to connect to the second VM in the network, <code>224</code> the third etc.</p>
<p>When logged in, look at <code>/var/log/cloud-init-output.log</code> and <code>/var/log/cloud-init.log</code>. You can find the metadata in <code>/var/lib/cloud/instance/</code>.</p>
<p>You will not have much time to look around -- the still running <code>api_monitor.sh</code> script does continue and clean things up again. So you might want to suspend it with <code>^Z</code> (and continue it later with <code>fg</code>). Another option is to not stop the regular monitoring, but start a second instance manually; see above notes for running multiple instances though. If you start a second instance manually against the same project, do NOT use the <code>run_CLOUDNAME.sh</code> script as it would do cleanup against the running instance, but rather copy the <code>api_monitor.sh</code> command line from the bottom (without the <code>exec</code>), reduce the iterations to a few (unless you need a lot to trigger the issue again) and attach <code>-w -1</code> to make the script stop its operation (and wait for Enter) once it hits an error. Of course, you still will face cleanup when the continuing main script hits its 200th iteration and you have chosen to run this second instance against the same project in the same cloud. After analyzing, do not forget to go back to the tmux window where the stopped script is running and do hit Enter, so it can continue and do its cleanup work.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="cleaning-things-up">Cleaning things up<a href="#cleaning-things-up" class="hash-link" aria-label="Direct link to Cleaning things up" title="Direct link to Cleaning things up" translate="no">​</a></h3>
<p>If you are unlucky, the script fails to clean something up. A volume may not have been named (because of a cinder failure) or all the logic may have gone wrong, e.g. the heuristic to avoid leaking floating IPs. You can try to clean this up using the normal openstack commands (or horizon dashboard).</p>
<p>There are a few things that may need support from a cloud admin:</p>
<ul>
<li class="">Volumes may end up permanently in a <code>deleting</code> or <code>reserved</code> state or may be <code>in-use</code>, attached to a VM that has long gone. The admin needs to set the state to <code>error</code> and then delete them.</li>
<li class="">Loadbalancers may end up in a <code>PENDING_XXX</code> state (<code>XXX</code> being <code>CREATE</code>, <code>UPDATE</code> or <code>DELETE</code>) without ever changing. This also needs the cloud admin to set the status to <code>ERROR</code>, so it can be cleaned up. amphorae are more prone to this than ovn LBs.</li>
</ul>
<p>More like these may happen, but those two are the only ones that have been observed to happen occasionally. Some services seem to be less robust than others against an event in the event queue (rabbitmq) being lost or an connection to be interrupted.</p>
<p><em>The source of this document can be found in the <a href="https://raw.githubusercontent.com/SovereignCloudStack/openstack-health-monitor/main/docs/Debian12-Install.md" target="_blank" rel="noopener noreferrer" class="">SovereignCloudStack/openstack-health-monitor</a> repository.</em></p>
<p><em>Author: SCS Community, License: CC by Attribution 4.0 International</em></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/SovereignCloudStack/docs/tree/main/docs/02-iaas/components/openstack-health-monitor.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#intro" class="table-of-contents__link toc-highlight">Intro</a></li><li><a href="#setting-up-the-driver-vm" class="table-of-contents__link toc-highlight">Setting up the driver VM</a><ul><li><a href="#internal-vs-external-monitoring" class="table-of-contents__link toc-highlight">Internal vs external monitoring</a></li><li><a href="#unprivileged-operation" class="table-of-contents__link toc-highlight">Unprivileged operation</a></li><li><a href="#driver-vm-via-openstack-cli" class="table-of-contents__link toc-highlight">Driver VM via openstack CLI</a></li><li><a href="#configuring-openstack-cli-on-the-driver-vm" class="table-of-contents__link toc-highlight">Configuring openstack CLI on the driver VM</a></li><li><a href="#custom-ca" class="table-of-contents__link toc-highlight">Custom CA</a></li></ul></li><li><a href="#your-first-api_monitorsh-iteration" class="table-of-contents__link toc-highlight">Your first <code>api_monitor.sh</code> iteration</a><ul><li><a href="#resource-impact-and-charging" class="table-of-contents__link toc-highlight">Resource impact and charging</a></li></ul></li><li><a href="#automating-startup-and-cleanup" class="table-of-contents__link toc-highlight">Automating startup and cleanup</a><ul><li><a href="#changing-parameters-and-restarting" class="table-of-contents__link toc-highlight">Changing parameters and restarting</a></li><li><a href="#multiple-instances" class="table-of-contents__link toc-highlight">Multiple instances</a></li></ul></li><li><a href="#alarming-and-logs" class="table-of-contents__link toc-highlight">Alarming and Logs</a><ul><li><a href="#email" class="table-of-contents__link toc-highlight">eMail</a></li><li><a href="#log-files" class="table-of-contents__link toc-highlight">Log files</a></li></ul></li><li><a href="#data-collection-and-dashboard" class="table-of-contents__link toc-highlight">Data collection and dashboard</a><ul><li><a href="#telegraf" class="table-of-contents__link toc-highlight">Telegraf</a></li><li><a href="#influxdb" class="table-of-contents__link toc-highlight">InfluxDB</a></li><li><a href="#add--s-cloudname-to-your-run_cloudnamesh-script" class="table-of-contents__link toc-highlight">Add <code>-S CLOUDNAME</code> to your <code>run_CLOUDNAME.sh</code> script</a></li><li><a href="#caddy-reverse-proxy" class="table-of-contents__link toc-highlight">Caddy (Reverse Proxy)</a></li><li><a href="#grafana" class="table-of-contents__link toc-highlight">Grafana</a></li></ul></li><li><a href="#maintenance" class="table-of-contents__link toc-highlight">Maintenance</a><ul><li><a href="#unattended-upgrades" class="table-of-contents__link toc-highlight">Unattended upgrades</a></li><li><a href="#sshd-setup" class="table-of-contents__link toc-highlight">sshd setup</a></li><li><a href="#updating-openstack-health-monitor" class="table-of-contents__link toc-highlight">Updating openstack-health-monitor</a></li><li><a href="#backup" class="table-of-contents__link toc-highlight">Backup</a></li></ul></li><li><a href="#troubleshooting" class="table-of-contents__link toc-highlight">Troubleshooting</a><ul><li><a href="#debugging-issues" class="table-of-contents__link toc-highlight">Debugging issues</a></li><li><a href="#analyzing-failures" class="table-of-contents__link toc-highlight">Analyzing failures</a></li><li><a href="#cleaning-things-up" class="table-of-contents__link toc-highlight">Cleaning things up</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs">Contribute</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://matrix.to/#/!TiDqlLmEUaXqTemaLc:matrix.org?via=matrix.org" target="_blank" rel="noopener noreferrer" class="footer__link-item">Matrix<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://fosstodon.org/@sovereigncloudstack" target="_blank" rel="noopener noreferrer" class="footer__link-item">Mastodon<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/SovereignCloudStack/docs" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Sovereign Cloud Stack, SCS and the logo are registered trademarks of the Open Source Business Alliance e.V. — Other trademarks are property of their respective owners.</div></div></div></footer></div>
</body>
</html>